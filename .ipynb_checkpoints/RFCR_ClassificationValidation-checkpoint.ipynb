{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import arcpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_featureclass_to_pddataframe(fc, fields_list=[\"*\"], remove_index=False):\n",
    "    \"\"\"\n",
    "    Converts a feature class to a pandas dataframe.\n",
    "    :param fc: Input feature class\n",
    "    :param fields_list: Optional parameter - defaults to wildcard [\"*\"]. Optionally include specific fields.\n",
    "    :param remove_index: Optional parameter - defaults to False. Remove index from output dataframe.\n",
    "    :return: Pandas dataframe of the feature class WITHOUT geometry and date attributes.\n",
    "    \"\"\"\n",
    "    # Gather a list of all field names if the user did not specify field inputs\n",
    "    if fields_list == [\"*\"]:\n",
    "        # Generate a valid list of field names that can be passed to the arcpy.FeatureClassToNumPyArray function\n",
    "        # (must filter out geometry and date fields!\n",
    "        fields_list = [field_object.name for field_object in arcpy.ListFields(fc) if field_object.type not in [\"Geometry\", \"Date\"]]\n",
    "\n",
    "    temp_array = arcpy.da.FeatureClassToNumPyArray(fc, fields_list)\n",
    "    df = pd.DataFrame(data=temp_array)\n",
    "    if remove_index:\n",
    "        df.reset_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to return result boolean val\n",
    "def return_boolean_result(known_val, predicted_val):\n",
    "\n",
    "    if predicted_val == known_val:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to determine confusion matrix values\n",
    "def return_confusion_matrix_val(known_val, predicted_val, positive_val, negative_val):\n",
    "    # Set true positive\n",
    "    if known_val == positive_val and predicted_val == positive_val:\n",
    "        return \"True Positive\" \n",
    "    \n",
    "    # Set true negative\n",
    "    if known_val == negative_val and predicted_val == negative_val:\n",
    "        return \"True Negative\"\n",
    "    \n",
    "    # Set false positive\n",
    "    if known_val == negative_val and predicted_val == positive_val:\n",
    "        return \"False Positive\" \n",
    "    \n",
    "    # Set false negative\n",
    "    if known_val == positive_val and predicted_val == negative_val:\n",
    "        return \"False Negative\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(tp, tn, fp, fn, round_val=4):\n",
    "    acc = round((tp + tn) / (tp + tn + fp + fn), round_val)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fscore(tp, fp, fn, round_val=4):\n",
    "    # Calculate precision and recall\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    # Handle division by zero\n",
    "    if precision == 0 and recall == 0:\n",
    "        f = \"N/A\"\n",
    "    else:\n",
    "        # Calculate f-score\n",
    "        f = round(2 * ( (precision * recall) / (precision + recall)), round_val)\n",
    "    return f, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mcc(tp, tn, fp, fn, round_val=4):\n",
    "    if tp+fp == 0 or tp+fn == 0 or tn+fp == 0 or tn+fn == 0:\n",
    "        mcc = \"N/A\"\n",
    "    else:\n",
    "        mcc = round(((tp*tn)-(fp*fn)) / math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)), round_val)\n",
    "    return mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the accuracy of the classification results from a run of \"Random Forest Classification and Regression\"\n",
    "def evaluate_classification(predicted_fc, known_val_field, predicted_val_field, trained_field=\"TRAINED_ID\", positive_val=1, negative_val=0, verbose=False):\n",
    "\n",
    "    # Bring necessary modules\n",
    "    import pandas as pd\n",
    "    import arcgis\n",
    "\n",
    "    # Extract the predicted feature class into a pandas dataframe\n",
    "    # df = arcgis.features.SpatialDataFrame().from_featureclass(predicted_fc)\n",
    "    df = convert_featureclass_to_pddataframe(predicted_fc)\n",
    "\n",
    "    # Find all the records that were not used to train (trained_field == 0)\n",
    "    df = df[(df[trained_field] == 0)]\n",
    "\n",
    "    # Get a total count\n",
    "    total_test_records = df.shape[0]\n",
    "    \n",
    "    # Compute a new boolean field 'result' and determine if prediction was correct\n",
    "    df['CORRECT'] = df.apply(lambda x: return_boolean_result(x[known_val_field], x[predicted_val_field]), axis=1)\n",
    "\n",
    "    # Get total count of correct answers\n",
    "    total_correct_records = df[(df['CORRECT']==1)].shape[0]\n",
    "   \n",
    "    # Calculate confusion matrix vals\n",
    "    df['CONFUSION_MATRIX_VAL'] = df.apply(lambda x: return_confusion_matrix_val(x[known_val_field], \n",
    "                                                                                x[predicted_val_field], \n",
    "                                                                                positive_val, \n",
    "                                                                                negative_val), axis=1) \n",
    "    \n",
    "    # Calculate totals from confusion matrix\n",
    "    total_tp = df[(df['CONFUSION_MATRIX_VAL'] == \"True Positive\")].shape[0]\n",
    "    total_tn = df[(df['CONFUSION_MATRIX_VAL'] == \"True Negative\")].shape[0]\n",
    "    total_fp = df[(df['CONFUSION_MATRIX_VAL'] == \"False Positive\")].shape[0]\n",
    "    total_fn = df[(df['CONFUSION_MATRIX_VAL'] == \"False Negative\")].shape[0]\n",
    "\n",
    "    # Calculate the derivations from the confusion matrix\n",
    "    # percent_correct = round(total_correct_records / total_test_records, 4)\n",
    "    accuracy = calculate_accuracy(total_tp, total_tn, total_fp, total_fn)\n",
    "    fscore, precision, recall = calculate_fscore(total_tp, total_fp, total_fn)\n",
    "    mcc = calculate_mcc(total_tp, total_tn, total_fp, total_fn)\n",
    "    \n",
    "    total_known_positive = df[(df[known_val_field]==positive_val)].shape[0]\n",
    "    total_known_negative = df[(df[known_val_field]==negative_val)].shape[0]\n",
    "    \n",
    "    tp_perc = round(total_tp / total_known_positive, 4) * 100\n",
    "    tn_perc = round(total_tn / total_known_negative, 4) * 100\n",
    "    fp_perc = round(total_fp / total_known_positive, 4) * 100\n",
    "    fn_perc = round(total_fn / total_known_negative, 4) * 100\n",
    "    \n",
    "    tp_total_perc = round(total_tp / total_test_records, 4) * 100\n",
    "    tn_total_perc = round(total_tn / total_test_records, 4) * 100\n",
    "    fp_total_perc = round(total_fp / total_test_records, 4) * 100\n",
    "    fn_total_perc = round(total_fn / total_test_records, 4) * 100\n",
    "    \n",
    "    print(\">>>> Classification Diagnostics <<<<\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nObservations: \")\n",
    "        print(\"\\tTotal Known Positives: {0}\".format(total_known_positive))\n",
    "        print(\"\\tTotal Known Negatives: {0}\".format(total_known_negative))\n",
    "\n",
    "        print(\"\\nClassification Results: \")\n",
    "        print(\"\\tTrue Positives: {0} ({1}%)\".format(total_tp, tp_perc))\n",
    "        print(\"\\tTrue Negatives: {0} ({1}%)\".format(total_tn, tn_perc))\n",
    "        print(\"\\tFalse Positives: {0} ({1}%)\".format(total_fp, fp_perc))\n",
    "        print(\"\\tFalse Negatives: {0} ({1}%)\".format(total_fn, fn_perc))\n",
    "\n",
    "\n",
    "    print(\"\\n\\tPrecision: {0}\".format(round(precision, 4)))\n",
    "    print(\"\\tRecall: {0}\".format(round(recall, 4)))\n",
    "\n",
    "    print(\"\\n\\tAccuracy: {0}\".format(accuracy))\n",
    "    print(\"\\tF-Score: {0}\".format(str(fscore)))\n",
    "    print(\"\\tMCC: {0}\".format(str(mcc)))\n",
    "    \n",
    "    return accuracy, fscore, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_true_positives(predicted_fc_lyr_name, known_val_field, predicted_val_field, trained_field=\"TRAINED_ID\", positive_val=1):\n",
    "\n",
    "    arcpy.management.SelectLayerByAttribute(predicted_fc_lyr_name, \n",
    "                                            \"NEW_SELECTION\", \n",
    "                                            \"{0} = 0 And {1} = {3} And {2} = {3}\".format(trained_field, \n",
    "                                                                                         known_val_field, \n",
    "                                                                                         predicted_val_field,\n",
    "                                                                                         positive_val), \n",
    "                                            None)\n",
    "\n",
    "\n",
    "def select_true_negatives(predicted_fc_lyr_name, known_val_field, predicted_val_field, trained_field=\"TRAINED_ID\", negative_val=0):\n",
    "\n",
    "    arcpy.management.SelectLayerByAttribute(predicted_fc_lyr_name, \n",
    "                                            \"NEW_SELECTION\", \n",
    "                                            \"{0} = 0 And {1} = {3} And {2} = {3}\".format(trained_field, \n",
    "                                                                                         known_val_field, \n",
    "                                                                                         predicted_val_field,\n",
    "                                                                                         negative_val), \n",
    "                                            None)\n",
    "\n",
    "def select_false_positives(predicted_fc_lyr_name, known_val_field, predicted_val_field, trained_field=\"TRAINED_ID\", positive_val=1, negative_val=0):\n",
    "\n",
    "    arcpy.management.SelectLayerByAttribute(predicted_fc_lyr_name, \n",
    "                                            \"NEW_SELECTION\", \n",
    "                                            \"{0} = 0 And {1} = {3} And {2} = {4}\".format(trained_field, \n",
    "                                                                                         known_val_field, \n",
    "                                                                                         predicted_val_field,\n",
    "                                                                                         positive_val,\n",
    "                                                                                         negative_val), \n",
    "                                            None)\n",
    "\n",
    "def select_false_negatives(predicted_fc_lyr_name, known_val_field, predicted_val_field, trained_field=\"TRAINED_ID\", positive_val=1, negative_val=0):\n",
    "\n",
    "    arcpy.management.SelectLayerByAttribute(predicted_fc_lyr_name, \n",
    "                                            \"NEW_SELECTION\", \n",
    "                                            \"{0} = 0 And {1} = {4} And {2} = {3}\".format(trained_field, \n",
    "                                                                                         known_val_field, \n",
    "                                                                                         predicted_val_field,\n",
    "                                                                                         positive_val,\n",
    "                                                                                         negative_val),\n",
    "                                            None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_fc = \"D:\\\\SpatialStats\\\\RandomForestTool\\\\RFCP_Exploration_SalesPrediction\\\\RFCP_Exploration_SalesPrediction.gdb\\\\RFCP_ArsonPrediction\"\n",
    "known_val_field = \"ARSON\"\n",
    "predicted_val_field = \"PREDICTED\"\n",
    "trained_field = \"TRAINED_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Classification Diagnostics <<<<\n",
      "\n",
      "Observations: \n",
      "\tTotal Known Positives: 499\n",
      "\tTotal Known Negatives: 5123\n",
      "\n",
      "Classification Results: \n",
      "\tTrue Positives: 58 (11.62%)\n",
      "\tTrue Negatives: 4753 (92.78%)\n",
      "\tFalse Positives: 370 (74.15%)\n",
      "\tFalse Negatives: 441 (8.61%)\n",
      "\n",
      "\tPrecision: 0.1355\n",
      "\tRecall: 0.1162\n",
      "\n",
      "\tAccuracy: 0.8557\n",
      "\tF-Score: 0.1251\n",
      "\tMCC: 0.0472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8557, 0.1251, 0.0472)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification(predicted_fc, known_val_field, predicted_val_field, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_fc = \"D:\\\\SpatialStats\\\\RandomForestTool\\\\RFCP_Exploration_SalesPrediction\\\\RFCP_Exploration_SalesPrediction.gdb\\\\RFCP_ArsonPrediction_OriginalVars\"\n",
    "known_val_field = \"ARSON\"\n",
    "predicted_val_field = \"PREDICTED\"\n",
    "trained_field = \"TRAINED_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Classification Diagnostics <<<<\n",
      "\n",
      "Observations: \n",
      "\tTotal Known Positives: 463\n",
      "\tTotal Known Negatives: 5159\n",
      "\n",
      "Classification Results: \n",
      "\tTrue Positives: 0 (0.0%)\n",
      "\tTrue Negatives: 5156 (99.94%)\n",
      "\tFalse Positives: 3 (0.65%)\n",
      "\tFalse Negatives: 463 (8.97%)\n",
      "\n",
      "\tPrecision: 0.0\n",
      "\tRecall: 0.0\n",
      "\n",
      "\tAccuracy: 0.9171\n",
      "\tF-Score: N/A\n",
      "\tMCC: -0.0069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9171, 'N/A', -0.0069)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification(predicted_fc, known_val_field, predicted_val_field, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
